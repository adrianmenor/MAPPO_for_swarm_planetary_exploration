# MAPPO
Multi-Agent PPO algorithm for swarm space exploration.
![Simulation](https://github.com/adrianmenor/MAPPO/assets/61149732/ad6f58aa-258f-4821-9bae-626788011c87)
